{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os # for handling the directory\n",
    "import shutil\n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = '/hhome/ps2g07/document_analysis/github/Project_Synthesis2-/Datasets/Publaynet/Publaynet_partition_1/Synthetics_DS_partition1/train.json'\n",
    "with open(train_json) as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "val_json = '/hhome/ps2g07/document_analysis/github/Project_Synthesis2-/Datasets/Publaynet/Publaynet_partition_1/Synthetics_DS_partition1/val.json'\n",
    "with open(val_json) as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an auxiliary dictionary to map image_id to filename\n",
    "def obtain_new_dict(data):\n",
    "    id_to_filename = {image['id']: {'file_name': image['file_name'], 'width': image['width'], 'height': image['height']} for image in data['images']}\n",
    "\n",
    "    # Initialize your dictionary with filenames as keys\n",
    "    new_dict = {values['file_name']: {'width': values['width'], 'height': values['height'], 'images': []} for values in id_to_filename.values()}\n",
    "\n",
    "    # Iterate through annotations a single time, adding them to the corresponding list in new_dict\n",
    "    for annotation in data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        if image_id in id_to_filename:  # Check if the image_id exists in the mapping\n",
    "            filename = id_to_filename[image_id]['file_name']\n",
    "            new_dict[filename]['images'].append(annotation)\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "new_dict_train = obtain_new_dict(train_data)\n",
    "new_dict_val = obtain_new_dict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt_files(data, folder_path):\n",
    "    for file_name in data.keys():\n",
    "        with open(f'{folder_path}{file_name[:-4]}.txt', 'w') as f:\n",
    "            for annotation in data[file_name]['images']:\n",
    "                x, y, w, h = annotation['bbox']\n",
    "                x_center = x + w/2\n",
    "                y_center = y + h/2\n",
    "                x_center /= data[file_name]['width']\n",
    "                y_center /= data[file_name]['height']\n",
    "                w /= data[file_name]['width']\n",
    "                h /= data[file_name]['height']\n",
    "                class_id = annotation['category_id'] - 1\n",
    "                f.write(f\"{class_id} {x_center} {y_center} {w} {h}\\n\")\n",
    "\n",
    "# Create the txt files for the train and val datasets\n",
    "create_txt_files(new_dict_train, '/hhome/ps2g07/document_analysis/github/Project_Synthesis2-/Nil/yolo_dir/data_part1/labels/train/')\n",
    "create_txt_files(new_dict_val, '/hhome/ps2g07/document_analysis/github/Project_Synthesis2-/Nil/yolo_dir/data_part1/labels/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'supercategory': '', 'id': 1, 'name': 'text'},\n",
       " {'supercategory': '', 'id': 2, 'name': 'title'},\n",
       " {'supercategory': '', 'id': 3, 'name': 'list'},\n",
       " {'supercategory': '', 'id': 4, 'name': 'table'},\n",
       " {'supercategory': '', 'id': 5, 'name': 'figure'},\n",
       " {'supercategory': '', 'id': 6, 'name': 'signature'},\n",
       " {'supercategory': '', 'id': 7, 'name': 'stamp'},\n",
       " {'supercategory': '', 'id': 8, 'name': 'qr'},\n",
       " {'supercategory': '', 'id': 9, 'name': 'barcode'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'supercategory': '', 'id': 1, 'name': 'text'},\n",
    " {'supercategory': '', 'id': 2, 'name': 'title'},\n",
    " {'supercategory': '', 'id': 3, 'name': 'list'},\n",
    " {'supercategory': '', 'id': 4, 'name': 'table'},\n",
    " {'supercategory': '', 'id': 5, 'name': 'figure'},\n",
    " {'supercategory': '', 'id': 6, 'name': 'signature'},\n",
    " {'supercategory': '', 'id': 7, 'name': 'stamp'},\n",
    " {'supercategory': '', 'id': 8, 'name': 'qr'},\n",
    " {'supercategory': '', 'id': 9, 'name': 'barcode'}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocTR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
